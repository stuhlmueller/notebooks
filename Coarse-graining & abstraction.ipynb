{
 "metadata": {
  "name": "Coarse-graining & abstraction"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Coarse-graining & abstraction\n",
      "\n",
      "Notes on [Detecting emergent processes in cellular automata with excess information](http://dl.dropbox.com/u/5874168/emergent.pdf) by David Balduzzi."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction\n",
      "\n",
      "- \"a principled method for identifying which spatiotemporal scale best expresses a cellular automaton's dynamics\n",
      "- Examples: Game of Life, Hopfield networks\n",
      "- Contributions:\n",
      "  - coarse-graining procedure for expressing a CA's dynamics at different scales\n",
      "  - a method for distinguishing good coarse-grainings from bad coarse-grainings (\"those that generate more information better express automaton's dynamics\")\n",
      "- Advantages of the approach in this paper:\n",
      "  - coarse-graining is compositional: coarse-graining is itself an automaton\n",
      "- based on selectivity, not predictability (\"the latter requires a model and a decision of what to predict\", whereas $ei$ and $\\xi$ depend only on the actual process)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Terms and concepts\n",
      "\n",
      "- occasion:\n",
      "  - space-time coordinate (cell 5,5 at time 3)\n",
      "- unit: \n",
      "  - a grouping of occasions (some 3x3 grid at time 4)\n",
      "  - don't have to be made up of adjacent occasions\n",
      "- channel:\n",
      "  - the thing that allows units to interact\n",
      "  - \"transparent occasions whose outputs are marginalized over\"\n",
      "- ground: \n",
      "  - some occasions\n",
      "  - \"fixes the initial condition of the coarse-grained system\"\n",
      "- effective information $ei$: \n",
      "  - quantifies how selectively a system's output depends on its inputs (high if few inputs cause \"the output\")\n",
      "- excess information $\\xi$:\n",
      "  - measure difference between information generated by system and subsystems\n",
      "- emergence:\n",
      "  - system is emergent if better expressed at coarser scale\n",
      "  - \"more than the sum of its parts\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Probabilistic cellular automata\n",
      "\n",
      "#### Game of Life\n",
      "\n",
      "1. Any live cell with fewer than two live neighbours dies, as if caused by under-population.\n",
      "2. Any live cell with two or three live neighbours lives on to the next generation.\n",
      "3. Any live cell with more than three live neighbours dies, as if by overcrowding.\n",
      "4. Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.\n",
      "\n",
      "#### Hopfield network\n",
      "\n",
      "$$ \n",
      "a_i \\leftarrow 1 \\mbox { if }\\sum_{j}{w_{ij}s_j} \\gt \\theta_i\n",
      "$$\n",
      "\n",
      "#### Abstract definition\n",
      "\n",
      "- A cellular automaton is a finite directed graph $X$ with vertices $V_X$ (\"occasions\", spacetime coords)\n",
      "- Each occasion $v_l$ has \n",
      "  - a finite output alphabet $A_l$\n",
      "  - a Markov matrix (or *mechanism*) $p_l(a_l|s_l)$\n",
      "    - here, $s_l$ is the \"combined alphabet of occasions targetting $v_l$\n",
      "    - specifies the probability of output $a_l$ given input $s_l$\n",
      "- The input (and output) alphabet of the automaton $X$ is the product of the alphabets of its occasions\n",
      "- Inputs are causal interventions via $do(-)$\n",
      "- Output (of occasion? or CA?) is calculated via $p_l(a_l|do(s_l))$\n",
      "- Cellular automata are essentially Bayes nets (usually dynamical Bayes nets)\n",
      "    - could by cyclic, but we can restrict to acyclic ones\n",
      "    - to initialize, we set all nodes to some value; then compute for ecah node distribution on new value given parent values\n",
      "    - in the following $do$ notation (mostly?) serves to address issues with cycles, so we may be able to ignore it\n",
      "\n",
      "#### Subsystem\n",
      "\n",
      "- A *subsystem* $X$ of a cellular automaton $Y$ is a subgraph that contains a subset of $Y$'s nodes and a subset of the transitions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class CellularAutomaton(object):\n",
      "    def __init__(self, nodes, transitions):\n",
      "        self.nodes = nodes\n",
      "        self.transitions = transitions\n",
      "        \n",
      "def coarse_grain(X, Y, K):\n",
      "    \"\"\"\n",
      "    Inputs:\n",
      "       Y: a cellular automaton\n",
      "       X: a subsystem to coarse-grain\n",
      "       K: a tuple (G, C, Us, sG) where (G, C, Us) partitions the units of X          \n",
      "          G: the ground, the set of occasions whose outputs are fixed\n",
      "          C: the channel\n",
      "          U1 .. Un: the coarse units\n",
      "    Output:\n",
      "       X_K, a new coarse-grained (sub)automaton\n",
      "    \"\"\"\n",
      "    # 1. marginalize over extrinsic inputs\n",
      "    # 2. fix the ground\n",
      "    # 3. marginalize over the channel\n",
      "    # 4. compute the effective graph of coarse-graining X_K\n",
      "    # 5. compute macro-alphabets of units in X_K\n",
      "    return X_K"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Algorithm for coarse-graining\n",
      "\n",
      "- Input:\n",
      "  - a subsystem $X \\subset Y$ and data $K$ and produces a new cellular automaton $X_K$.\n",
      "  - Data $K$ consists of\n",
      "      - a partition of $X$'s occasions into ground $G$, channel $C$, and units $U_1 \\dots U_N$\n",
      "         - \"The ground specifies occasions whose outputs are fixed -- the initial condition $s^G$\"\n",
      "         - The channel specifies unobserved occasions: interactions between units propagate across the channel\n",
      "      - ground output $s^G$\n",
      "- Output:\n",
      "  - a new coarse-grained (sub-)automaton $X_K$\n",
      "  - the nodes of automaton $X_K$ are the units $U_1 \\dots U_N$\n",
      "- Algorithm (idea):\n",
      "  1. Marginalize over extrinsic inputs\n",
      "     - we are only interested in *internal* processing and hence treat external inputs as noise\n",
      "     - for each occasion, there are inputs from within and without $X$\n",
      "     - we compute new transitions $p_l(a_l|s^X_l)$ by marginalizing over external inputs using the uniform distribution\n",
      "  2. Fix the ground\n",
      "     - Ground outputs are fixed in the new coarse-grained system\n",
      "     - So just set the transitions such that they don't explicitly depend on the ground states\n",
      "  3. Marginalize over the channel\n",
      "     - We assume that the coarse-grained automaton is fully observed, just like the original automaton\n",
      "     - To achieve this, we marginalize over the channel occasions s.t. the channel is reduced to transition probabilities between new coarse units\n",
      "  4. Compute the effective graph of coarse-graining $X_K$\n",
      "     - we can compute the micro-alphabet of the new units (product) and the mechanism (fixing ground, marginalizing over channels and extrinsic inputs)\n",
      "     - we can reduce the number of edges if some don't make a difference: if conditioned on all other inputs $\\bar x_{in}$, there are no values $a_k$, $a'_k$ such that $p_{U_l}(a_l|\\bar x_{in}, a_k) \\neq p_{U_l}(a_l|\\bar x_{in}, a'_k)$, then we don't need to draw an edge.\n",
      "  5. Compute macro-alphabets of units in $X_K$\n",
      "     - the above coarse-graining procedure can make outputs indistinguishable\n",
      "     - $b$ and $b'$ are indistinguishable if \n",
      "          - (a) for all $a$ and $c$, $p(a|b,c) = p(a|b',c)$.\n",
      "          - (b) $p(b|c) = p(b'|c)$ for all $c$\n",
      "     - in this case, we pick one element from each equivalence class"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Information\n",
      "\n",
      "- Given subsystem $m$ of $X$, let $p_m(x_{out}|x_{in})$, or just $m$, denote its $mechanism$ or Markov matrix.\n",
      "- The mechanisms is computed by taking the Markov matrix of each occasion in $X$, marginalizing over extrinsic inputs, and taking the product (i.e. $x_{in}$ and $x_{out}$ are not really all inputs and outputs, only the ones within the subsystem).\n",
      "- By input we mean \"we set the values of all nodes\", and by output we mean (a sample from) the conditional distribution on each node, given its parents.\n",
      "\n",
      "#### Effective information\n",
      "\n",
      "- $ei$ quantifies how selectively a mechanism discriminates between inputs when choosing an output. (\"how sharp the functional dependencies leading to an output are\")\n",
      "- Intuition: given this output, how much do we learn about the input of the mechanism? If we learn a lot (i.e. the distribution on inputs is very different from the uniform distribution), then this combination of output and mechanism has high effective information.\n",
      "- actual repertoire $\\hat p_m(X_{in}|x_{out})$:\n",
      "  - the distribution on inputs that causes $m$ to choose $x_{out}$:\n",
      "    $$\\hat p_m(x_{in}|x_{out}) := \\frac{p_m(x_{out}|do(x_{in}))}{p(x_{out})} \\cdot p_{unif}(x_{in})$$\n",
      "  - for deterministic functions, the actual repertoire assigns uniform probability to each element in the preimage of $x_{out}$, $0$ otherwise\n",
      "- Effective information is defined for a particular output $x_{out}$ and \n",
      "mechanism $m$. Definition:\n",
      "\n",
      "  $$ ei(m, x_{out}):= KL \\left [ \\hat p_m(X_{in}|x_{out}) \\mid | p_{unif}(X_{in}) \\right ] $$\n",
      "\n",
      "- For deterministic functions, $ei(f, x_{out})$ is the log of the ratio of the number of all possible inputs to the preimage size. For one-to-one functions, $ei$ is simply the log of the support.\n",
      "- \"Given that we observe all values of a Bayes net, how much do we learn about the values that must have caused this setting?\" -- a bit strange, so maybe the metaphor doesn't quite work?\n",
      "\n",
      "#### Excess information\n",
      "\n",
      "- quantifies how much more information a mechanism generates than the \"sum\" of its submechanisms (\"synergy of internal dependencies\")\n",
      "- idea: we compare how much we learn about the input given the whole system to how much we learn about the input of each subsystem if we partition our system (choosing the partition that works best) -- i.e., formalize how strongly integrated a system is\n",
      "- Given:\n",
      "  - a subsystem with mechanism $m$\n",
      "  - partition $P$ of the occasions in $src(m)$ (?)\n",
      "  - output $x_{out}$\n",
      "- Definition I:\n",
      "$$ \\xi(m, P, x_{out}) := ei(m, x_{out}) - \\sum_j ei(m^j, x_{out}) $$\n",
      "- If we are not given a partition, use the minimum information bipartition (to reduce computational burden---**huh!?** only in examples or also defn?), i.e., the partition $P^{MIP}$ that minimizes normalized excess information\n",
      "- Excess information is negative if any decomposition of the system generates more information than the whole\n",
      "- Not clear that this is a natural definition (that $ei$ and sum of $ei$s can be compared, that $0$ is at a meaningful point)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Neurons\n",
      "\n",
      "- Figure 3\n",
      "- Independent vs redundant vs synergistic information\n",
      "    - redundant: what we learn about input by looking at one mechanism is about as much as we learn by looking at whole system, so $ei$ less than sum of $ei$s of subsystems\n",
      "    - synergistic: the outputs of both neurons together allow us to tell what the input was, but looking at one neuron doesn't constrain input strongly\n",
      "- Categorizations overlap more / less than expected"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application: Game of Life\n",
      "\n",
      "- gliders as \"emergent\" mechanism\n",
      "- Figure 4: coarse graining of glider positions into two 3x3 squares\n",
      "- Intuitively, what I want to do: turn grid into 3x3 cells, time into groups of 4, and give new deterministic transition function that moves point one diagonally down. Is this captured here? Example shows that one simple coarse-graining that captures glider does better than other such simple grainings\n",
      "- Figure 5: \"the size of the macro-alphabet decreases exponentially as the distance between units increases, stabilizing at 5 macro-outputs: the 4 glider phases and a large equivalence class of outputs that result in a blank target unit\"\n",
      "  - idea: for short distances, there are other patterns that can still affect target unit?\n",
      "  - don't follow the camera analogy here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Application: Hopfield networks\n",
      "\n",
      "- for any initial condition, Hopfield networks tend to one of few attractors\n",
      "- can coarse-graining reveal attractor dynamics?\n",
      "- here: coupled Hopfield networks\n",
      "- \"intuitively, $A$ only exerts a strong force on $B$ once it has stalled in an  attractor and before $B$ transitions to the same attractor. Is the force $A$ exerts on $B$ quantitatively detectable?\"\n",
      "- INT coarse-graining captures $B$'s effect on itself, EXT capture's $A$'s effect on B\n",
      "- fixing the ground internalizes outside effects into the new coarse-grained system\n",
      "- according to this model, attractors are not emergend phenomena due to the high redundancy (leading to little excess information) -- need to better understand this\n",
      "- \"our analysis suggets that *transitions* between attractors are the most interesting emergent behaviors in coupled Hopfield networks\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Emergence\n",
      "\n",
      "- forming units out of disjoint collections of occasions yields $\\xi = 0$\n",
      "- \"boring\"/redundant units such as blank grid patches or attractors have $\\xi \\lt 0$\n",
      "- we therefore use $\\xi$ as a candidate for quantifying emergent processes\n",
      "- idea: a system is emergent if its dynamics are better expressed at a coarser spatiotemporal scale\n",
      "- emergent units should generate more excess information, and have more excess information generated about them, than their sub-units\n",
      "- emergent units should generate more excess information than neighboring units (why?)\n",
      "- Preliminaries:\n",
      "    - $$ {src}_{v_l} = \\{ v_l \\} \\cup \\{ v_k | k \\rightarrow l \\} $$\n",
      "    - ${trg}_{v_l}$ similarly\n",
      "    - $J$ is a subgraining of $K$, $J \\prec K$, means: for each unit in $J$ there is a unit in $K$ such that $U_j \\subsetneq U_k$\n",
      "          - i.e. we consider particular partitions\n",
      "  - Excess information for comparing a unit with its subgrains:\n",
      "$$ \\xi_{K/J}(m, x_{out}) := ei_{\\bar K}(m, x_{out}) - \\sum_{v_j \\in J} ei_{\\hat J}(m^j, x_{out}) $$\n",
      "  - $m^j = m \\cap src_{v_j}$\n",
      "  - $ei_{\\bar K}$ means $ei$ using micro-alphabets (why?)\n",
      "- Definition: A coarse-graining $K$ is emergent for automaton $X$ and output $x_{out}$ if it satisfies:\n",
      "    - E1: Each unit $U_l \\in K$ generates excess information about its sources and has excess information generated about by its targets, relative to subgrains $J \\prec K$:\n",
      "        - $ \\xi_{J/K}({src}_{U_l}, x_{out}) \\gt 0$\n",
      "        - $ \\xi_{J/K}({trg}_{U_l}, x_{out}) \\gt 0$\n",
      "        - This means: interactions between units and their sources/targets are \"synergistic\" (not redundant, or independent)\n",
      "            - why such a local measure?\n",
      "    - E2: there is an emergent subgrain $J \\prec K$ s.t. (i) every unit of $K$ contains a unit of $J$ and (ii) neighbors $K'$ of $K$ satisfy $\\xi_{J/{K'}}({src}_{U'}, x_{out}) \\leq \\xi_{J/{K}}({src}_{U}, x_{out})$.\n",
      "        - This means: unclear\n",
      "- Definition of neighborship, omitted\n",
      "- The best graining for $X$ and $x_{out}$ can be found by argmaxing over normalized $\\xi$, restricted to emergent $K$s.\n",
      "    - Normalization biases towards grainings (i) whose MIPs contain few, symmetric parts (why?) and (ii) that have simpler macro-alphabets (\"more symbolic\")\n",
      "    - What if we just argmax over all $K$s?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Discussion\n",
      "\n",
      "- Coarse-graining and emergence are context-dependent\n",
      "- Definition of emergence provisional\n",
      "- Number of possible coarse-grainings is vast\n",
      "- Manipulating macro-alphabets provides a method for approximate computations on large-scale systems\n",
      "- Coarse-grainings can be adjusted (MCMC-like?)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Things to think about \n",
      "\n",
      "- What does this formalism apply to? Why should we care about this?\n",
      "- Intuitively, what should we take away from this?\n",
      "- Can we translate this into the language of graphical models? Is it useful there?\n",
      "- How would we approach this problem?\n",
      "- What is the goal of this procedure? How do we evaluate whether it meets its goal?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Other questions\n",
      "\n",
      "- why talk about alphabets instead of random variables and their support?\n",
      "- how does the abstract CA definition relate to Bayesian networks?\n",
      "  - can be cyclic\n",
      "- what exactly is the role of the initial condition and of the ground?\n",
      "- what is the intuitive meaning of effective information?\n",
      "- what does it mean for a coarse-graining to generate information?\n",
      "- intuitively, structures that are close in space and time should be grouped together; how does the formalism proposed in this paper give rise to this effect?\n",
      "- is \"based on selectivity, not predictability\" actually an advantage? should the concept of emergence depend on the observer?\n",
      "- \"we are only interested in *internal* processing and hence treat external inputs as noise\" -- is this good?\n",
      "- multiple effective graphs / sequentialization matters?\n",
      "- uniform distribution, maximum entropy, and justified/unjustified choice\n",
      "- for $ei$, why compare to uniform distribution as opposed to comparison to the distributions that could have occurred with other outputs?\n",
      "- should $ei$ depend on the size of the support?\n",
      "- what exactly is the normalization of excess information doing?\n",
      "- according to this model, Hopfield attractors are not emergent phenomena due to the high redundancy -- do we want this, or does this point to flaws in the formalism?\n",
      "- if it had turned out that attractors have $\\xi \\gt 0$, would the paper have argued that attractors are an emergent phenomenon and that this is therefore evidence for the definition? conservation of expected evidence\n",
      "- the definition of emergence is rather complex. what if some conditions were left out?\n",
      "- we're grouping by occasions (variables), not by values -- is that a problem?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}